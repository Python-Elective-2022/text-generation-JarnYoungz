{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5135792e-cf77-4c95-bda9-af6f20b7c5cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-20 04:36:10--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.200.102, 74.125.200.113, 74.125.200.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.200.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vcbvmtdmeaiulqc37gq7du22jouvnmq6/1679286900000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=8c6ad9c2-058b-4805-b6ca-02fabb35a62b [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-03-20 04:36:13--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vcbvmtdmeaiulqc37gq7du22jouvnmq6/1679286900000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=8c6ad9c2-058b-4805-b6ca-02fabb35a62b\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 74.125.24.132, 2404:6800:4003:c03::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|74.125.24.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M   179MB/s    in 0.4s    \n",
            "\n",
            "2023-03-20 04:36:14 (179 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5c8396-aaa2-4812-edfd-0ddc81d9f4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-fbdddccf8583>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b814b3e5-37d2-4288-8441-d5936839dcf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e21ea80-0e89-400b-980f-c7cebb9936d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 14s 80ms/step - loss: 5.9711 - accuracy: 0.0353\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 5.4377 - accuracy: 0.0353\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 5.3676 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 5.3110 - accuracy: 0.0399\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.2399 - accuracy: 0.0404\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 5.1673 - accuracy: 0.0404\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 7ms/step - loss: 5.0974 - accuracy: 0.0409\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.0275 - accuracy: 0.0525\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.9583 - accuracy: 0.0555\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.8757 - accuracy: 0.0721\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 4.7902 - accuracy: 0.0757\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 4.6995 - accuracy: 0.0787\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.6153 - accuracy: 0.1034\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.5266 - accuracy: 0.1085\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.4318 - accuracy: 0.1216\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.3483 - accuracy: 0.1287\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 4.2508 - accuracy: 0.1367\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1538 - accuracy: 0.1640\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 4.0797 - accuracy: 0.1655\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.9976 - accuracy: 0.1862\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.9233 - accuracy: 0.1887\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.8496 - accuracy: 0.2114\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.7773 - accuracy: 0.2270\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.7127 - accuracy: 0.2361\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.6675 - accuracy: 0.2346\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5856 - accuracy: 0.2573\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5355 - accuracy: 0.2583\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4999 - accuracy: 0.2730\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4218 - accuracy: 0.2841\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3526 - accuracy: 0.3047\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3089 - accuracy: 0.3103\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2378 - accuracy: 0.3214\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1881 - accuracy: 0.3370\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 3.1301 - accuracy: 0.3421\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.0711 - accuracy: 0.3567\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.0153 - accuracy: 0.3673\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.9622 - accuracy: 0.3809\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9112 - accuracy: 0.3971\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8708 - accuracy: 0.4006\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.8184 - accuracy: 0.4057\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7689 - accuracy: 0.4132\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7117 - accuracy: 0.4279\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6683 - accuracy: 0.4379\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6352 - accuracy: 0.4501\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6193 - accuracy: 0.4551\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5760 - accuracy: 0.4586\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5155 - accuracy: 0.4753\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4582 - accuracy: 0.4813\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4239 - accuracy: 0.4960\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3845 - accuracy: 0.5040\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3557 - accuracy: 0.5111\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3010 - accuracy: 0.5146\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2508 - accuracy: 0.5348\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2280 - accuracy: 0.5318\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.1995 - accuracy: 0.5424\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1925 - accuracy: 0.5328\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1508 - accuracy: 0.5444\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0843 - accuracy: 0.5540\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.0287 - accuracy: 0.5676\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.0866 - accuracy: 0.5631\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.0056 - accuracy: 0.5772\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.9974 - accuracy: 0.5747\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0388 - accuracy: 0.5782\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9417 - accuracy: 0.5903\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8777 - accuracy: 0.6034\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8291 - accuracy: 0.6125\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7988 - accuracy: 0.6191\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7693 - accuracy: 0.6221\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7405 - accuracy: 0.6352\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7105 - accuracy: 0.6377\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6830 - accuracy: 0.6463\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6607 - accuracy: 0.6569\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6300 - accuracy: 0.6604\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6042 - accuracy: 0.6700\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5901 - accuracy: 0.6650\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5604 - accuracy: 0.6741\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5357 - accuracy: 0.6806\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5061 - accuracy: 0.6882\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4893 - accuracy: 0.6927\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4713 - accuracy: 0.6988\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4456 - accuracy: 0.7008\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4289 - accuracy: 0.7023\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4158 - accuracy: 0.6998\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4038 - accuracy: 0.7079\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3733 - accuracy: 0.7109\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.3585 - accuracy: 0.7175\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.3364 - accuracy: 0.7175\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.3086 - accuracy: 0.7286\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.3026 - accuracy: 0.7220\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2876 - accuracy: 0.7321\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2819 - accuracy: 0.7195\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2734 - accuracy: 0.7296\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2386 - accuracy: 0.7427\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2003 - accuracy: 0.7482\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1805 - accuracy: 0.7497\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1628 - accuracy: 0.7583\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.1461 - accuracy: 0.7629\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1285 - accuracy: 0.7684\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.1116 - accuracy: 0.7709\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0993 - accuracy: 0.7725\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0862 - accuracy: 0.7825\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0708 - accuracy: 0.7815\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.0595 - accuracy: 0.7795\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.0544 - accuracy: 0.7704\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0378 - accuracy: 0.7881\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0223 - accuracy: 0.7876\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0072 - accuracy: 0.7881\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9971 - accuracy: 0.7841\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9849 - accuracy: 0.7997\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9679 - accuracy: 0.7967\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.9592 - accuracy: 0.7962\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9435 - accuracy: 0.8012\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9239 - accuracy: 0.8058\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9191 - accuracy: 0.8032\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9239 - accuracy: 0.7972\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9102 - accuracy: 0.8052\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9113 - accuracy: 0.7957\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8813 - accuracy: 0.8118\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8595 - accuracy: 0.8189\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8597 - accuracy: 0.8138\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8965 - accuracy: 0.8027\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8852 - accuracy: 0.8012\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8437 - accuracy: 0.8143\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8133 - accuracy: 0.8254\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8027 - accuracy: 0.8214\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7906 - accuracy: 0.8249\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7790 - accuracy: 0.8325\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7743 - accuracy: 0.8280\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7702 - accuracy: 0.8310\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7639 - accuracy: 0.8355\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7689 - accuracy: 0.8285\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7542 - accuracy: 0.8315\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7401 - accuracy: 0.8345\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7313 - accuracy: 0.8330\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7169 - accuracy: 0.8396\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7118 - accuracy: 0.8396\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6997 - accuracy: 0.8446\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6941 - accuracy: 0.8486\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7045 - accuracy: 0.8421\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6899 - accuracy: 0.8446\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.6900 - accuracy: 0.8491\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6890 - accuracy: 0.8456\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6564 - accuracy: 0.8577\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6529 - accuracy: 0.8577\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6442 - accuracy: 0.8623\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6410 - accuracy: 0.8633\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6429 - accuracy: 0.8597\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6406 - accuracy: 0.8567\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6324 - accuracy: 0.8618\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6183 - accuracy: 0.8648\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6070 - accuracy: 0.8628\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6025 - accuracy: 0.8648\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5992 - accuracy: 0.8688\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6015 - accuracy: 0.8643\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5961 - accuracy: 0.8678\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5959 - accuracy: 0.8643\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6154 - accuracy: 0.8552\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6012 - accuracy: 0.8633\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5789 - accuracy: 0.8678\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5685 - accuracy: 0.8703\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5798 - accuracy: 0.8653\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5666 - accuracy: 0.8708\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5489 - accuracy: 0.8739\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5452 - accuracy: 0.8744\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5391 - accuracy: 0.8729\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5334 - accuracy: 0.8739\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5252 - accuracy: 0.8789\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5217 - accuracy: 0.8799\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5160 - accuracy: 0.8789\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5099 - accuracy: 0.8789\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5059 - accuracy: 0.8814\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5154 - accuracy: 0.8759\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5069 - accuracy: 0.8794\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5056 - accuracy: 0.8809\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5173 - accuracy: 0.8734\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5167 - accuracy: 0.8744\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4974 - accuracy: 0.8789\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4908 - accuracy: 0.8845\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4922 - accuracy: 0.8784\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4884 - accuracy: 0.8809\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4815 - accuracy: 0.8824\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4701 - accuracy: 0.8829\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4684 - accuracy: 0.8905\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4584 - accuracy: 0.8865\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4521 - accuracy: 0.8895\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4482 - accuracy: 0.8850\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4441 - accuracy: 0.8865\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4404 - accuracy: 0.8880\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.4379 - accuracy: 0.8890\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4345 - accuracy: 0.8860\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4326 - accuracy: 0.8870\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4309 - accuracy: 0.8890\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4292 - accuracy: 0.8910\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4275 - accuracy: 0.8900\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4377 - accuracy: 0.8870\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4372 - accuracy: 0.8840\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4328 - accuracy: 0.8845\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4410 - accuracy: 0.8799\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4431 - accuracy: 0.8824\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4279 - accuracy: 0.8870\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "709e5227-8d1b-43b8-cc0f-462cfc978047"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a1d0022-94a7-4084-bf64-2f97e16912b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 658ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "im feeling chills me again cause i do what could i do what could i do what could be such a joe all enough on hes casting seems deal hes talk joe joe joe joe joe shame throwing shame throwing shame shadow shame shadow shame shadow shadow shame throwing shame shadow warm throwing shame throwing shame throwing shame shadow shame shadow shadow shame throwing shame shadow shame shadow shadow shame throwing shame shadow shame shadow shame shadow shadow throwing shame throwing shame shadow shame shadow shadow shame throwing shame shadow throwing shame throwing shame shadow shame shadow shame shadow shadow shame throwing shame\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}